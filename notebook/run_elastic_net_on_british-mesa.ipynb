{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/vol/bmd/yanyul/UKB/ptrs-tf/models'\n",
    "population = 'British'  # for test 'Chinese'\n",
    "pred_expr_name = 'MESA_CAU'\n",
    "logfile = f'/vol/bmd/yanyul/UKB/ptrs-tf/models/elastic_net_{pred_expr_name}.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "import util_ElasticNet, lib_LinearAlgebra, util_hdf5, lib_ElasticNet, lib_Checker\n",
    "import util_misc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py, yaml, functools\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "lib_LinearAlgebra = reload(lib_LinearAlgebra)\n",
    "util_ElasticNet = reload(util_ElasticNet)\n",
    "util_hdf5 = reload(util_hdf5)\n",
    "lib_ElasticNet = reload(lib_ElasticNet)\n",
    "lib_Checker = reload(lib_Checker)\n",
    "util_misc = reload(util_misc)\n",
    "import util_hdf5\n",
    "import logging, sys\n",
    "import seaborn as sns\n",
    "logging.basicConfig(\n",
    "    level = logging.INFO, \n",
    "#     stream = sys.stderr,\n",
    "    filename = logfile,\n",
    "    format = '%(asctime)s  %(message)s',\n",
    "    datefmt = '%Y-%m-%d %I:%M:%S %p'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis overview\n",
    "\n",
    "Building PTRS using Elastic Net. \n",
    "\n",
    "1. Split British data into 3 sets: training, test, validation.\n",
    "2. Train a sequence of elastic net predictors along regularization path using British training data.\n",
    "3. Repeat step 2 for $\\alpha = 0.1, 0.5, 0.9$\n",
    "\n",
    "More about setting up: \n",
    "$\\frac{\\lambda_{max}}{\\lambda_{min}} = 10^6$. \n",
    "nlambda = 50.\n",
    "Number of max iteration is 100.\n",
    "Batch size is roughly 1/5 of sample size.\n",
    "\n",
    "For MESA dataset, we take the genes in both EUR models and AFR+HIS models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to British data\n",
    "mesa_cau = f'/vol/bmd/yanyul/UKB/predicted_expression_tf2/ukb_imp_x_MESA_CAU_{population}.hdf5'\n",
    "mesa_afhi = f'/vol/bmd/yanyul/UKB/predicted_expression_tf2/ukb_imp_x_MESA_AFHI_{population}.hdf5'\n",
    "\n",
    "# extract the gene names \n",
    "with h5py.File(mesa_cau, 'r') as f:\n",
    "    genes_cau = f['columns_x'][...].astype(str)\n",
    "with h5py.File(mesa_afhi, 'r') as f:\n",
    "    genes_afhi = f['columns_x'][...].astype(str)\n",
    "\n",
    "# get the genes occur in both models\n",
    "x_indice_cau, x_indice_afhi = util_misc.intersect_indice(genes_cau, genes_afhi)\n",
    "\n",
    "# data scheme specifying which are traits and covariates\n",
    "scheme_yaml = '../misc_files/data_scheme.yaml'\n",
    "\n",
    "# loading names of traits/covariates\n",
    "# the order is matched with the data being loaded\n",
    "feature_dic = util_hdf5.read_yaml(scheme_yaml)\n",
    "with h5py.File(mesa_cau, 'r') as f:\n",
    "    features = f['columns_y'][:].astype('str')\n",
    "    sample_size = f['y'].shape[0]\n",
    "    y = f['y'][:]\n",
    "covar_indice = np.where(np.isin(features, feature_dic['covar_names']))[0]\n",
    "trait_indice = np.where(np.isin(features, feature_dic['outcome_names']))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Features in order')\n",
    "logging.info(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data_scheme for training\n",
    "batch_size_to_load = 2 ** 12  # int(sample_size / 8) + 1\n",
    "logging.info(f'batch_size in {population} set is {batch_size_to_load}')\n",
    "data_scheme, sample_size = util_hdf5.build_data_scheme(\n",
    "    mesa_cau, \n",
    "    scheme_yaml, \n",
    "    batch_size = batch_size_to_load, \n",
    "    inv_norm_y = True,\n",
    "    x_indice = x_indice_cau\n",
    ")\n",
    "\n",
    "# set validation and test set as the first and second batch\n",
    "# dataset_valid = data_scheme.dataset.take(1)\n",
    "data_scheme.dataset = data_scheme.dataset.skip(1)\n",
    "# dataset_test = data_scheme.dataset.take(1)\n",
    "data_scheme.dataset = data_scheme.dataset.skip(1)\n",
    "batch_size = int(sample_size / 4) + 1\n",
    "data_scheme.dataset = data_scheme.dataset.unbatch().batch(batch_size)\n",
    "# dataset_insample = data_scheme.dataset.take(1)\n",
    "ntrain = sample_size - batch_size_to_load * 2\n",
    "train_batch = batch_size\n",
    "logging.info(f'train_batch = {train_batch}, ntrain = {ntrain}')\n",
    "# data_scheme.dataset = data_scheme.dataset.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving lambda_seq\n",
      "Saving beta_hat_path\n",
      "Saving covar_hat_path\n",
      "Saving intercept_path\n",
      "Saving normalizer\n",
      "Saving alpha\n",
      "Saving data_scheme.dataset\n",
      "Saving data_scheme.X_index\n",
      "Saving data_scheme.Y_index\n",
      "Saving data_scheme.outcome_indice\n",
      "Saving data_scheme.covariate_indice\n",
      "Saving data_scheme.x_indice\n",
      "Saving data_scheme.num_predictors\n",
      "Saving lambda_seq\n",
      "Saving beta_hat_path\n",
      "Saving covar_hat_path\n",
      "Saving intercept_path\n",
      "Saving normalizer\n",
      "Saving alpha\n",
      "Saving data_scheme.dataset\n",
      "Saving data_scheme.X_index\n",
      "Saving data_scheme.Y_index\n",
      "Saving data_scheme.outcome_indice\n",
      "Saving data_scheme.covariate_indice\n",
      "Saving data_scheme.x_indice\n",
      "Saving data_scheme.num_predictors\n",
      "Saving lambda_seq\n",
      "Saving beta_hat_path\n",
      "Saving covar_hat_path\n",
      "Saving intercept_path\n",
      "Saving normalizer\n",
      "Saving alpha\n",
      "Saving data_scheme.dataset\n",
      "Saving data_scheme.X_index\n",
      "Saving data_scheme.Y_index\n",
      "Saving data_scheme.outcome_indice\n",
      "Saving data_scheme.covariate_indice\n",
      "Saving data_scheme.x_indice\n",
      "Saving data_scheme.num_predictors\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0.1, 0.5, 0.9]\n",
    "learning_rate = 1\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    logging.info('alpha = {} starts'.format(alpha))\n",
    "    lambda_init_dict = {\n",
    "        'data_init': None, \n",
    "        'prefactor_of_lambda_max': 1.5,\n",
    "        'lambda_max_over_lambda_min': 1e6,\n",
    "        'nlambda': 50\n",
    "    }\n",
    "    updater = lib_ElasticNet.ProximalUpdater(learning_rate = learning_rate, line_search = True)\n",
    "    update_dic = {\n",
    "        'updater': updater,\n",
    "        'update_fun': updater.proximal_train_step\n",
    "    }\n",
    "    my_stop_rule = functools.partial(lib_Checker.diff_stop_rule, threshold = 1e-3)\n",
    "    ny = len(data_scheme.outcome_indice)\n",
    "    elastic_net_estimator = lib_LinearAlgebra.ElasticNetEstimator(\n",
    "        data_scheme,\n",
    "        alpha,\n",
    "        normalizer = True,\n",
    "        learning_rate = learning_rate,\n",
    "        lambda_init_dict = lambda_init_dict,\n",
    "        updater = update_dic\n",
    "    )\n",
    "    checker = [ lib_Checker.Checker(ntrain, train_batch, lib_Checker.my_stat_fun, my_stop_rule) \n",
    "               for i in range(ny) ]\n",
    "\n",
    "    elastic_net_estimator.solve(checker, nepoch = 100, logging = logging)\n",
    "    \n",
    "    \n",
    "    outfile = f'{output_dir}/elastic_net_{pred_expr_name}_alpha_{alpha}_{population}.hdf5'\n",
    "    logging.info(f'alpha = {alpha} saving to {outfile}')\n",
    "    elastic_net_estimator.minimal_save(outfile)\n",
    "    logging.info('alpha = {} ends'.format(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving lambda_seq\n",
      "Saving beta_hat_path\n",
      "Saving covar_hat_path\n",
      "Saving intercept_path\n",
      "Saving normalizer\n",
      "Saving alpha\n",
      "Saving data_scheme.dataset\n",
      "Saving data_scheme.X_index\n",
      "Saving data_scheme.Y_index\n",
      "Saving data_scheme.outcome_indice\n",
      "Saving data_scheme.covariate_indice\n",
      "Saving data_scheme.x_indice\n",
      "Saving data_scheme.num_predictors\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### for test below ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elastic_net_estimator.minimal_save('test.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data_scheme for training\n",
    "# batch_size_to_load = 2 ** 8 # 2 ** 12  # int(sample_size / 8) + 1\n",
    "# logging.info(f'batch_size in {population} set is {batch_size_to_load}')\n",
    "# data_scheme, sample_size = util_hdf5.build_data_scheme(\n",
    "#     mesa_afhi, \n",
    "#     scheme_yaml, \n",
    "#     batch_size = batch_size_to_load, \n",
    "#     inv_norm_y = True,\n",
    "#     x_indice = x_indice_afhi  # x_indice_afhi\n",
    "# )\n",
    "\n",
    "# # set validation and test set as the first and second batch\n",
    "# # dataset_valid = data_scheme.dataset.take(1)\n",
    "# data_scheme.dataset = data_scheme.dataset.skip(1)\n",
    "# # dataset_test = data_scheme.dataset.take(1)\n",
    "# data_scheme.dataset = data_scheme.dataset.skip(1)\n",
    "# batch_size = int(sample_size / 4) + 1\n",
    "# data_scheme.dataset = data_scheme.dataset.unbatch().batch(batch_size)\n",
    "# # dataset_insample = data_scheme.dataset.take(1)\n",
    "# ntrain = sample_size - batch_size_to_load * 2\n",
    "# train_batch = batch_size\n",
    "# logging.info(f'train_batch = {train_batch}, ntrain = {ntrain}')\n",
    "# # data_scheme.dataset = data_scheme.dataset.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lib_LinearAlgebra.ElasticNetEstimator('', None, minimal_load = True)\n",
    "# model.minimal_load('test.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.data_scheme.x_indice = data_scheme.x_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model.predict_x(data_scheme.dataset, model.beta_hat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # out = o1\n",
    "# fig, aes = plt.subplots(nrows = 3, ncols = 3, figsize = (15, 10))\n",
    "# seq = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 16, 19]\n",
    "# for i in range(3):\n",
    "#     for j in range(3):\n",
    "#         idx = seq[(i * 3 + j)] + 40\n",
    "#         if idx < len(model.lambda_seq[0]):\n",
    "#             for k in range(1):\n",
    "#                 aes[i][j].scatter(out['y'][:,k], out['y_pred_from_x'][:, k, idx])\n",
    "#             aes[i][j].set_title(\n",
    "#                 'lambda = ' + \"{:.3E} cor = {:.3E}\".format(\n",
    "#                     model.lambda_seq[0][idx], \n",
    "#                     np.corrcoef(out['y'][:, 0], out['y_pred_from_x'][:, 0, idx])[0,1]\n",
    "#                 ) # + '\\n' +\n",
    "# #                 'lambda = ' + \"{:.3E} cor = {:.3E}\".format(\n",
    "# #                     model_list[alpha].lambda_seq[1][idx], \n",
    "# #                     np.corrcoef(out['y'][:, 1], out['y_pred_from_x'][:, 1, idx])[0,1]\n",
    "# #                 )\n",
    "#             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
