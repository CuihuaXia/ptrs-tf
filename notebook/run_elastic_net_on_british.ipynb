{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = '/vol/bmd/yanyul/UKB/ptrs-tf/models/elastic_net.log'\n",
    "output_dir = '/vol/bmd/yanyul/UKB/ptrs-tf/models'\n",
    "population = 'British'  # for test 'Chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "import util_ElasticNet, lib_LinearAlgebra, util_hdf5, lib_ElasticNet, lib_Checker\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py, yaml, functools\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "lib_LinearAlgebra = reload(lib_LinearAlgebra)\n",
    "util_ElasticNet = reload(util_ElasticNet)\n",
    "util_hdf5 = reload(util_hdf5)\n",
    "lib_ElasticNet = reload(lib_ElasticNet)\n",
    "lib_Checker = reload(lib_Checker)\n",
    "import util_hdf5\n",
    "import logging, sys\n",
    "import seaborn as sns\n",
    "logging.basicConfig(\n",
    "    level = logging.INFO, \n",
    "#     stream = sys.stderr,\n",
    "    filename = logfile,\n",
    "    format = '%(asctime)s  %(message)s',\n",
    "    datefmt = '%Y-%m-%d %I:%M:%S %p'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis overview\n",
    "\n",
    "Building PTRS using Elastic Net. \n",
    "\n",
    "1. Split British data into 3 sets: training, test, validation.\n",
    "2. Train a sequence of elastic net predictors along regularization path using British training data.\n",
    "3. Repeat step 2 for $\\alpha = 0.1, 0.5, 0.9$\n",
    "\n",
    "More about setting up: \n",
    "$\\frac{\\lambda_{max}}{\\lambda_{min}} = 10^6$. \n",
    "nlambda = 50.\n",
    "Number of max iteration is 100.\n",
    "Batch size is roughly 1/5 of sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to British data\n",
    "hdf5_british = f'/vol/bmd/yanyul/UKB/predicted_expression_tf2/ukb_imp_x_ctimp_Whole_Blood_{population}.hdf5'\n",
    "\n",
    "# data scheme specifying which are traits and covariates\n",
    "scheme_yaml = '../misc_files/data_scheme.yaml'\n",
    "\n",
    "# loading names of traits/covariates\n",
    "# the order is matched with the data being loaded\n",
    "feature_dic = util_hdf5.read_yaml(scheme_yaml)\n",
    "with h5py.File(hdf5_british, 'r') as f:\n",
    "    features = f['columns_y'][:].astype('str')\n",
    "    sample_size = f['y'].shape[0]\n",
    "    y = f['y'][:]\n",
    "covar_indice = np.where(np.isin(features, feature_dic['covar_names']))[0]\n",
    "trait_indice = np.where(np.isin(features, feature_dic['outcome_names']))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Features in order')\n",
    "logging.info(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data_scheme for training\n",
    "batch_size_to_load = 2 ** 12  # int(sample_size / 8) + 1\n",
    "logging.info(f'batch_size in {population} set is {batch_size_to_load}')\n",
    "data_scheme, sample_size = util_hdf5.build_data_scheme(\n",
    "    hdf5_british, \n",
    "    scheme_yaml, \n",
    "    batch_size = batch_size_to_load, \n",
    "    inv_norm_y = True\n",
    ")\n",
    "\n",
    "# set validation and test set as the first and second batch\n",
    "# dataset_valid = data_scheme.dataset.take(1)\n",
    "data_scheme.dataset = data_scheme.dataset.skip(1)\n",
    "# dataset_test = data_scheme.dataset.take(1)\n",
    "data_scheme.dataset = data_scheme.dataset.skip(1)\n",
    "batch_size = int(sample_size / 4) + 1\n",
    "data_scheme.dataset = data_scheme.dataset.unbatch().batch(batch_size)\n",
    "# dataset_insample = data_scheme.dataset.take(1)\n",
    "ntrain = sample_size - batch_size_to_load * 2\n",
    "train_batch = batch_size\n",
    "logging.info(f'train_batch = {train_batch}, ntrain = {ntrain}')\n",
    "# data_scheme.dataset = data_scheme.dataset.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving lambda_seq\n",
      "Saving beta_hat_path\n",
      "Saving covar_hat_path\n",
      "Saving intercept_path\n",
      "Saving normalizer\n",
      "Saving alpha\n",
      "Saving data_scheme.dataset\n",
      "Saving data_scheme.X_index\n",
      "Saving data_scheme.Y_index\n",
      "Saving data_scheme.outcome_indice\n",
      "Saving data_scheme.covariate_indice\n",
      "Saving data_scheme.num_predictors\n",
      "Saving lambda_seq\n",
      "Saving beta_hat_path\n",
      "Saving covar_hat_path\n",
      "Saving intercept_path\n",
      "Saving normalizer\n",
      "Saving alpha\n",
      "Saving data_scheme.dataset\n",
      "Saving data_scheme.X_index\n",
      "Saving data_scheme.Y_index\n",
      "Saving data_scheme.outcome_indice\n",
      "Saving data_scheme.covariate_indice\n",
      "Saving data_scheme.num_predictors\n",
      "Saving lambda_seq\n",
      "Saving beta_hat_path\n",
      "Saving covar_hat_path\n",
      "Saving intercept_path\n",
      "Saving normalizer\n",
      "Saving alpha\n",
      "Saving data_scheme.dataset\n",
      "Saving data_scheme.X_index\n",
      "Saving data_scheme.Y_index\n",
      "Saving data_scheme.outcome_indice\n",
      "Saving data_scheme.covariate_indice\n",
      "Saving data_scheme.num_predictors\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0.1, 0.5, 0.9]\n",
    "learning_rate = 1\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    logging.info('alpha = {} starts'.format(alpha))\n",
    "    lambda_init_dict = {\n",
    "        'data_init': None, \n",
    "        'prefactor_of_lambda_max': 1.5,\n",
    "        'lambda_max_over_lambda_min': 1e6,\n",
    "        'nlambda': 50\n",
    "    }\n",
    "    updater = lib_ElasticNet.ProximalUpdater(learning_rate = learning_rate, line_search = True)\n",
    "    update_dic = {\n",
    "        'updater': updater,\n",
    "        'update_fun': updater.proximal_train_step\n",
    "    }\n",
    "    my_stop_rule = functools.partial(lib_Checker.diff_stop_rule, threshold = 1e-3)\n",
    "    ny = len(data_scheme.outcome_indice)\n",
    "    elastic_net_estimator = lib_LinearAlgebra.ElasticNetEstimator(\n",
    "        data_scheme,\n",
    "        alpha,\n",
    "        normalizer = True,\n",
    "        learning_rate = learning_rate,\n",
    "        lambda_init_dict = lambda_init_dict,\n",
    "        updater = update_dic\n",
    "    )\n",
    "    checker = [ lib_Checker.Checker(ntrain, train_batch, lib_Checker.my_stat_fun, my_stop_rule) \n",
    "               for i in range(ny) ]\n",
    "\n",
    "    elastic_net_estimator.solve(checker, nepoch = 100, logging = logging)\n",
    "    \n",
    "    \n",
    "    outfile = f'{output_dir}/elastic_net_alpha_{alpha}_{population}.hdf5'\n",
    "    logging.info(f'alpha = {alpha} saving to {outfile}')\n",
    "    elastic_net_estimator.minimal_save(outfile)\n",
    "    logging.info('alpha = {} ends'.format(alpha))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
