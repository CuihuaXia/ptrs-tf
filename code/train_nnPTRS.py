import argparse
parser = argparse.ArgumentParser(prog='train_nnPTRS.py', description='''
    Train neural net (CNN or MLP) using tensorflow 2.0 keras
''')
parser.add_argument('--training-set', required=True, help='''
    Predicted expression matrix (HDF5 generated by predixcan_prediction.py)
''')
parser.add_argument('--data-scheme-yaml', required=True, help='''
    YAML of data scheme
''')
parser.add_argument('--model-yaml', default=None, help='''
    Model YAML (should match with `--model-type`)
    Default: None (linear model)
    If set 'baseline', it means None as well
''')
parser.add_argument('--model-type', required=True, help='''
    CNN or MLP
''')
parser.add_argument('--batch-size', required=True, type=int, help='''
    Batch size in gradient descent
''')
parser.add_argument('--residual-mode', , action='store_true', help='''
    If specified, it first run LS and fit the residual without covariates.
''')
# parser.add_argument('--num-epoch', required=True, type=int, help='''
#     Number of epoch
# ''')
# parser.add_argument('--optimizer', required=True, help='''
#     SGD, Adam, ...
# ''')
parser.add_argument('--valid-and-test-size', default=4096, type=int, help='''
    Size of test and validation set (default = 4096). 
    Extract validation set and then test set.
''')
parser.add_argument('--output-prefix', required=True, help='''
    Prefix of output.
    Will generate [prefix].final.hdf5
    And intermediate files [prefix].[phase-name].hdf5
''')
parser.add_argument('--phase-yaml', default=None, help='''
    YAML structure:
    phase1:
        optimizer: 'Adam'
        epoch: 10
    phase2: 
        ...
    
    Default structure:
    phase1:
        optimizer: 'Adam'
        epoch: 20
''')
args = parser.parse_args()

import lib_LinearAlgebra, lib_cnnPTRS, util_hdf5, util_misc, util_Stats, util_train
import tensorflow as tf
import numpy as np
import pandas as pd
import h5py
import logging, sys

logging.basicConfig(
    level = logging.INFO, 
    stream = sys.stderr, 
    format = '%(asctime)s  %(message)s',
    datefmt = '%Y-%m-%d %I:%M:%S %p'
)

# args manipulation
if args.model_yaml == 'baseline':
    args.model_yaml = None

# Load spatial information
logging.info('Loading spatial information: get gene annotation')
df_gene = pd.read_table('https://bitbucket.org/yanyul/rotation-at-imlab/raw/85a3fbe8f08df7c67265fed69569b7ea554d4e12/data/annotations_gencode_v26.tsv')
df_gene['tss'] = df_gene[['start', 'end', 'strand']].apply(lambda x: util_train.get_tss(x.start, x.end, x.strand), axis = 1)
df_gene['chr_num'] = df_gene[['chromosome']].apply(lambda x: util_train.chr2num(x.chromosome), axis = 1)
df_gene.sort_values(['chr_num', 'tss'], ascending = [True, True], inplace = True) 
df_gene = df_gene.reset_index(drop = True)
df_gene['rank'] = df_gene.index

logging.info('Loading spatial information: join with training data')
with h5py.File(args.training_set, 'r') as f:
    col_genes = f['columns_x'][...]
col_genes_cleaned = [ i.astype(str).split('.')[0] for i in col_genes ]
df_col_genes = pd.DataFrame({'gene_id': col_genes_cleaned, 'col_idx': [ i for i in range(len(col_genes_cleaned)) ]})
df_gene_joined = df_gene.join(df_col_genes.set_index('gene_id'), on = 'gene_id')
df_gene_joined = df_gene_joined.loc[df_gene_joined['gene_id'].isin(df_col_genes['gene_id'].to_list())].reset_index(drop = True)

x_indice = [ int(i) for i in df_gene_joined['col_idx'].to_list() ]

# Load training set and do splitting
logging.info('Loading training set and splitting')
hdf5_test = args.training_set
scheme_yaml = args.data_scheme_yaml
split_size = args.valid_and_test_size  
logging.info(f'Split size in the training set is {split_size}')
data_scheme, sample_size = util_hdf5.build_data_scheme(
    hdf5_test, 
    scheme_yaml, 
    batch_size = split_size, 
    inv_norm_y = True 
    # x_indice = x_indice
)

data_scheme.x_indice = x_indice

logging.info('Set validation and test set as the first and second splits')
if args.residual_mode is True:
    # save for future use
    dataset_full = data_scheme.dataset
dataset_valid = data_scheme.dataset.take(1)
data_scheme.dataset = data_scheme.dataset.skip(1)
dataset_test = data_scheme.dataset.take(1)
data_scheme.dataset = data_scheme.dataset.skip(1)
dataset_insample = data_scheme.dataset.take(1)

batch_size = args.batch_size
logging.info(f'Set batch size = {batch_size}')
data_scheme.dataset = data_scheme.dataset.unbatch().batch(batch_size)

# If --residual-mode run the following to compute residual 
# and prepare tf.Dataset
logging.info('** In residual mode: start')
ls_ptrs = lib_LinearAlgebra.LeastSquaredEstimator(data_scheme, intercept = True, normalizer = True)
logging.info('** In residual mode: solving least squares')
ls_ptrs.solve()
logging.info('** In residual mode: predicting for the full dataset')
o = ls_ptrs.predict(dataset_full)
logging.info('** In residual mode: computing residual')
residual = o['y'] - o['y_pred']
logging.info('** In residual mode: REDO building Dataset')
data_scheme, sample_size = util_hdf5.build_data_scheme_with_preset_y(
    hdf5_test, 
    scheme_yaml, 
    residual,
    batch_size = split_size
) 

data_scheme.x_indice = x_indice

dataset_valid = data_scheme.dataset.take(1)
data_scheme.dataset = data_scheme.dataset.skip(1)
dataset_test = data_scheme.dataset.take(1)
data_scheme.dataset = data_scheme.dataset.skip(1)
dataset_insample = data_scheme.dataset.take(1)

# Prepare validation and insample Tensor
logging.info('Prepare validation and insample Tensors')
ele_insample = util_misc.get_inputs_and_y(dataset_insample, data_scheme.get_num_outcome())
ele_valid = util_misc.get_inputs_and_y(dataset_valid, data_scheme.get_num_outcome())

# Build Keras model
logging.info('Building model: {}'.format(args.model_type))
if args.model_yaml is not None:
    cnn_model = util_misc.load_ordered_yaml(args.model_yaml)
else:
    cnn_model = None
temp_placeholder = args.output_prefix + '.empty.h5'
if args.model_type == 'MLP' or args.model_yaml is None:
    nn = lib_cnnPTRS.mlpPTRS(cnn_model, data_scheme, temp_placeholder, normalizer = True)
elif args.model_type == 'CNN':
    nn = lib_cnnPTRS.cnnPTRS(cnn_model, data_scheme, temp_placeholder, normalizer = True)
nn.model.summary()
nn.add_logger(logging)
nn.minimal_save(temp_placeholder)


# Prepare normalizer
logging.info('Pre-computing normalizer')
norm, norm_v, norm_i = nn.prep_train(ele_valid, ele_insample) 

# Training
logging.info('Prepare phase yaml')
phase_dic = util_train.get_phase(args.phase_yaml)
prev_opt = ''
for phase in phase_dic.keys():
    logging.info(f'@ Phase {phase} start')
    if 'optimizer' not in phase_dic[phase] or 'epoch' not in phase_dic[phase]:
        logging.info('@ Wrong phase. Skip.')
        continue
    nn.temp_path = args.output_prefix + '.' + phase + '.h5'
    if phase_dic[phase]['optimizer'] != prev_opt:
        logging.info(f'@@ Phase {phase}: building optimizer and graph')
        optimizer = util_train.str2optimizer(phase_dic[phase]['optimizer'])
        mytrain = nn.train_func()
    
    # self, optimizer, num_epoch, ele_valid, normalizer = None, normalizer_valid = None, var_list = v, ele_insample = None, normalizer_insample = None
    logging.info('@@ Phase {}: start to run with optimizer {}'.format(phase, phase_dic[phase]['optimizer']))
    mytrain(
        nn, 
        optimizer, 
        phase_dic[phase]['epoch'], 
        ele_valid, 
        normalizer = norm, 
        normalizer_valid = norm_v, 
        ele_insample = ele_insample, 
        normalizer_insample = norm_i
    )
    
logging.info('Saving final model')
nn.minimal_save(args.output_prefix, '.final.h5')
